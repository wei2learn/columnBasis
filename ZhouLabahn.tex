%% LyX 2.0.5.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[twoside]{sig-alternate}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{color}
\usepackage{verbatim}
\usepackage{prettyref}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\floatname{algorithm}{Algorithm}
\usepackage{algorithmic}
\newcommand{\forbody}[1]{ #1 \ENDFOR }
\newcommand{\ifbody}[1]{ #1  \ENDIF}
\newcommand{\whilebody}[1]{ #1  \ENDWHILE}
\renewcommand{\algorithmicprint}{\textbf{draw}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

 \newtheorem{thm}{Theorem}[section]
 \newtheorem{lem}[thm]{Lemma}
 \newtheorem{defn}[thm]{Definition}
 \newtheorem{rem}[thm]{Remark}
 \newtheorem{exmp}[thm]{Example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\usepackage{yjsco}\journal{JournalofSymbolicComputation}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}\renewcommand{\algorithmicensure}{\textbf{Output:}}
%\renewcommand{\algorithmicensure}{\textbf{if}}\renewcommand{\algorithmicensure}{\textbf{Uses:}}

%\def\diag{\mbox{diag}}\def\cdeg{\qopname\relax n{cdeg}}
\def\MM{\qopname\relax n{MM}}\def\M{\qopname\relax n{M}}
%\def\ord{\qopname\relax n{ord}}

\def\StorjohannTransform{\qopname\relax n{StorjohannTransform}}\def\TransformUnbalanced{\qopname\relax n{TransformUnbalanced}}\def\rowDimension{\qopname\relax n{rowDimension}}\def\columnDimension{\qopname\relax n{columnDimension}}\DeclareMathOperator{\re}{rem}\DeclareMathOperator{\coeff}{coeff}\DeclareMathOperator{\lcoeff}{lcoeff}\def\mab{\qopname\relax n{orderBasis}}\def\mmab{\qopname\relax n{FastBasis}}\def\umab{\qopname\relax n{UnbalancedFastOrderBasis}}\newcommand{\bb}{\\}
\DeclareMathOperator{\mnbr}{MinimalKernelBasisReversed}
%\def\rdeg{\qopname\relax n{rdeg}}
%\DeclareMathOperator{\colBasis}{colBasis}
\DeclareMathOperator{\colBasis}{ColumnBasis}
\def\mnb{\qopname\relax n{MinimalKernelBasis ~ }}
%\DeclareMathOperator{\mnbr}{DualMinimaKernelBasis}


\newcommand{\arne}[1]{{\color{blue}\it {\bf Arne:} #1 }}
\newcommand{\wei}[1]{{\color{red}\it {\bf Wei:} #1}}
\newcommand{\george}[1]{{\color{green}\it {\bf George:} #1}}
\def\newblock{\hskip .11em plus .33em minus .07em}


\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\K}{ {\rm K}}
\newcommand{\revCol}{ {\rm revCol}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\tbigO}{\widetilde{\mathcal{O}}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\ocL}{\overline{\mathcal{L}}}
\newcommand{\tcL}{\widetilde{\mathcal{L}}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\oA}{\overline{A}}
\newcommand{\GL}{{\rm GL}\,}
\newcommand{\rank}{{\rm rank}\,}
\newcommand{\cdeg}{{\rm cdeg}\,}
\newcommand{\rdeg}{{\rm rdeg}\,}
\newcommand{\diag}{{\rm diag}\,}
\newcommand{\val}{{\rm val}\,}
\newcommand{\ord}{{\rm ord}\,}
\newcommand{\abs}[1]{\lvert#1\rvert}


\newrefformat{eq}{\textup{(\ref{#1})}}
\newrefformat{lem}{Lemma \ref{#1}}
\newrefformat{cla}{Claim \ref{#1}}
\newrefformat{thm}{Theorem \ref{#1}}
\newrefformat{cha}{Chapter \ref{#1}}
\newrefformat{sec}{Section \ref{#1}}
\newrefformat{rem}{Remark \ref{#1}}
\newrefformat{fac}{Fact \ref{#1}}
\newrefformat{sub}{Subsection \ref{#1}}
\newrefformat{cor}{Corollary \ref{#1}}
\newrefformat{cond}{Condition \ref{#1}}
\newrefformat{con}{Conjecture \ref{#1}}
\newrefformat{def}{Definition \ref{#1}}
\newrefformat{pro}{Proposition \ref{#1}}
\newrefformat{alg}{Algorithm \ref{#1}}
\newrefformat{exm}{Example \ref{#1}}
\newrefformat{line}{line \ref{#1}}
\newrefformat{tab}{Table \ref{#1} on page \pageref{#1}}
\newrefformat{fig}{Figure \ref{#1} on page \pageref{#1}}

\makeatother

\begin{document}
% --- Author Metadata here ---


%\conferenceinfo{ISSAC'09,} {July 28--31, 2009, Seoul, Republic of Korea.} 
%\CopyrightYear{2009}
%\crdata{978-1-60558-609-0/09/07} 


\numberofauthors{1}


\author{\alignauthor Wei Zhou and George Labahn\\
 \affaddr{Cheriton School of Computer Science}\\
 \affaddr{University of Waterloo}, \\
 \affaddr{Waterloo, Ontario, Canada}\\
 \email{\{w2zhou,glabahn\}@uwaterloo.ca} }


\date{{\normalsize{{\today \quad{}:: \timeofday}}}}


\title{Computation of Column Bases of Polynomial Matrices}
\maketitle
\begin{abstract}
Given a matrix of univariate polynomials over a field $\mathbb{K}$,
its columns generate a $\mathbb{K}\left[x\right]$-module. % consisting of all $\mathbb{K}\left[x\right]$-linear combination of these columns. 
%This module is used for such important computations as matrix GCDs and matrix normal forms for %polynomial matrices.
We call any basis of this module a column basis of the given matrix.
Matrix gcds and matrix normal forms are examples of such module bases.
In this paper we present a deterministic algorithm for the computation
of a column basis of an $m\times n$ input matrix with $m\le n$.
If $s$ is the average column degree of the input matrix, this algorithm
computes a column basis with a cost of $O^{\sim}\left(nm^{\omega-1}s\right)$
field operations in $\mathbb{K}$. Here the soft-$O$ notation is
Big-$O$ with log factors removed while $\omega$ is the exponent
of matrix multiplication. Note that the average column degree $s$
is bounded by the commonly used matrix degree that is also the maximum
column degree of the input matrix. 
\end{abstract}
% A category with the (minimum) three required fields
%\category{I.1.2}{Symbolic and Algebraic Manipulation}{Algorithms}%[Algebraic algorithms]
%A category including the fourth, optional field follows...
%\category{F.2.2}{Analysis of Algorithms and }{Nonnumerical Algorithms and Problems}%[Computations on discrete structures]


%\terms{Algorithms, Theory}


%\keywords{Order basis, Complexity}


\vspace{1mm}
 \textbf{Categories and Subject Descriptors:} I.1.2 {{[}{Symbolic
and Algebraic Manipulation}{]}}: {Algorithms}; F.2.2 {{[}{Analysis
of Algorithms and Problem Complexity}{]}}: {Nonnumerical Algorithms
and Problems}

\noindent \vspace{1mm}
 \textbf{General Terms:} Algorithms, Theory

\noindent \vspace{1mm}
 \textbf{Keywords:} Order Basis, Kernel basis, Nullspace basis, Column
Basis


\section{\label{sec:Matrix-GCD}Introduction}

\noindent In this paper, we consider the problem of efficiently computing
a column basis of a polynomial matrix $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$
with $n\ge m$. A column basis of $\mathbf{F}$ is a basis for the
$\mathbb{K}\left[x\right]$-module 
\[
\left\{ \mathbf{F}\mathbf{p}~|~\mathbf{p}\in\mathbb{K}\left[x\right]^{n}~\right\} ~.
\]
Such a basis can be represented as a full rank matrix $\mathbf{T}\in\mathbb{K}\left[x\right]^{m\times r}$
whose columns are the basis elements. A column basis is not unique
and indeed any column basis right multiplied by a unimodular polynomial
matrix gives another column basis. As a result, a column basis can
have arbitrarily high degree. In this paper, the computed column basis
has column degrees bounded by the largest column degrees of the input
matrix.

Column bases are fundamental constructions in polynomial matrix algebra.
As an example, when the row dimension is one (i.e. $m=1$), then finding
a column basis coincides with finding a greatest common divisor (GCD)
of all the polynomials in the matrix. Similarly, the nonzero columns
of column reduced forms, Popov normal forms, and Hermite normal forms
are all column bases satisfying additional degree constraints. A column
reduced form gives a special column basis whose column degrees are
the smallest possible, while Popov and Hermite forms are special column
reduced or shifted column reduced forms satisfying additional conditions
that make them unique. Efficient column basis computation is thus
useful for fast computation for such core procedures as determining
matrix GCDs \cite{BL2000}, column reduced forms \cite{BVP:1988}
and Popov forms \cite{villard96} of $\mathbf{F}$ with any dimension
and rank. Column basis computation %in this paper 
also provides a deterministic alternative to randomized lattice compression
\cite{li:2006,storjohann-villard:2005}.

Column bases are produced by column reduced, Popov and Hermite forms
and considerable work has been done on computing such forms, for example
\cite{bcl:2006,Giorgi2003,GSSV2012,sarkar2011,SS2011}. However most
of these existing algorithms require that the input matrices be square
nonsingular and so start with existing column bases. It is however
pointed out in \cite{sarkar2011,SS2011} that randomization can be
used to relax the square nonsingular requirement.

To compute a column basis, we know from \cite{BL1997} that any matrix
polynomial $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$ can
be unimodularly transformed to a column basis by repeatedly working
with the leading column coefficient matrices. However this method
of computing a column basis can be expensive. Indeed one needs to
work with up to $\sum\vec{s}$ such coefficient matrices, which could
involve up to $\sum\vec{s}$ polynomial matrix multiplications, where
a sum without index denotes the sum of all entries.

In this paper we give a fast, deterministic algorithm for the computation
of a column basis for $\mathbf{F}$ having complexity $O^{\sim}\left(nm^{\omega-1}s\right)$
field operations in $\mathbb{K}$ with $s$ being the average column
degree of $\mathbf{F}$. Here the soft-$O$ notation is Big-$O$ with
log factors removed while $\omega$ is the exponent of matrix multiplication.
Our algorithm works for both rectangular and non-full column rank
matrices. Our method depends on kernel basis computation of $\mathbf{F}$
along with finding a factorization of the input matrix polynomial
into a column basis and a left kernel basis of a right kernel basis
of $\mathbf{F}$. Finding the right and left kernel basis then makes
use of the fast kernel basis and order basis algorithms from \cite{za2012}
and \cite{za2009,ZL2012}, respectively. 

The remainder of this paper is as follows. Basic definitions and preliminary
results on both kernel and order bases are given in the next section.
Section 3 provides the matrix factorization form of our input polynomial
matrix that forms the core of our procedure, with a column basis being
the left factor, and the right factor is a left kernel basis of a
right kernel basis of the input matrix. Section 4 provides an algorithm
for fast computation of a left kernel basis making use of order bases
computation with unbalanced shift. The column basis algorithm is given
in Section 5 with the following section giving details on how the
methods can be improved when the number of columns is significantly
larger than the number of rows. The paper ends with a conclusion along
with topics for future research.


\section{Preliminaries}

In this paper computational cost is analyzed by bounding the number
of arithmetic operations in the coefficient field $\mathbb{K}$ on
an algebraic random access machine. We assume the cost of multiplying
two polynomial matrices with dimension $n$ and degree $d$ is $O^{\sim}(n^{\omega}d)$
field operations, where the multiplication exponent $\omega$ is assumed
to satisfy $2<\omega\le3$. We refer to the book by \cite{vonzurgathen}
for more details and references about the cost of polynomial multiplication
and matrix multiplication.

In this section we first describe the notations used in this paper,
and then give the basic definitions and properties of {\em shifted
degree}, {\em order basis} and {\em kernel basis} for a matrix
of polynomials. These will be the building blocks used in our algorithm.


\subsection{Notations}

For convenience we adopt the following notations in this paper. 
\begin{description}
\item [{Comparing~Unordered~Lists}] For two lists $\vec{a}\in\mathbb{Z}^{n}$
and $\vec{b}\in\mathbb{Z}^{n}$, let $\bar{a}=\left[\bar{a}_{1},\dots,\bar{a}_{n}\right]\in\mathbb{Z}^{n}$
and $\bar{b}=\left[\bar{b}_{1},\dots,\bar{b}_{n}\right]\in\mathbb{Z}^{n}$
be the lists consists of the entries of $\vec{a}$ and $\vec{b}$
but sorted in increasing order. 
\[
\begin{cases}
\vec{a}\ge\vec{b} & \mbox{if }\bar{a}_{i}\ge\bar{b}_{i}\mbox{ for all }i\in\left[1,\dots n\right]\\
\vec{a}\le\vec{b} & \mbox{if }\bar{a}_{i}\le\bar{b}_{i}\mbox{ for all }i\in\left[1,\dots n\right]\\
\vec{a}>\vec{b} & \mbox{if }\vec{a}\ge\vec{b}\mbox{ and }\bar{a}_{j}>\bar{b}_{j}\mbox{ for at least one }j\in\left[1,\dots n\right]\\
\vec{a}<\vec{b} & \mbox{if }\vec{a}\le\vec{b}\mbox{ and }\bar{a}_{j}<\bar{b}_{j}\mbox{ for at least one }j\in\left[1,\dots n\right].
\end{cases}
\]
\begin{comment}
Summation~Notation For a list $\vec{a}=\left[a_{1},\dots,a_{n}\right]\in\mathbb{Z}^{n}$,
we write $\sum\vec{a}$ without index to denote the summation of all
entries in $\vec{a}$. 
\end{comment}
%\item [{}]~

\item [{Uniformly~Shift~a~List}] For a list $\vec{a}=\left[a_{1},\dots,a_{n}\right]\in\mathbb{Z}^{n}$
and $c\in\mathbb{Z}$, we write $\vec{a}+c$ to denote $\vec{a}+\left[c,\dots,c\right]=\left[a_{1}+c,\dots,a_{n}+c\right]$,
with subtraction handled similarly. 
\item [{Compare~a~List~with~a~Integer}] For %a list 
$\vec{a}=\left[a_{1},\dots,a_{n}\right]\in\mathbb{Z}^{n}$ and $c\in\mathbb{Z}$,
we write $\vec{a}<c$ to denote $\vec{a}<\left[c,\dots,c\right]$,
and similarly for $>,\le,\ge,=$. 
\end{description}

\subsection{Shifted Degrees}

Our methods depend extensively on the concept of {\em shifted}
degrees of polynomial matrices \cite{BLV:1999}. For a column vector
$\mathbf{p}=\left[p_{1},\dots,p_{n}\right]^{T}$ of univariate polynomials
over a field $\mathbb{K}$, its column degree, denoted by $\cdeg\mathbf{p}$,
is the maximum of the degrees of the entries of $\mathbf{p}$, that
is, 
\[
\cdeg~\mathbf{p}=\max_{1\le i\le n}\deg p_{i}.
\]
The \emph{shifted column degree} generalizes this standard column
degree by taking the maximum after shifting the degrees by a given
integer vector that is known as a \emph{shift}. More specifically,
the shifted column degree of $\mathbf{p}$ with respect to a shift
$\vec{s}=\left[s_{1},\dots,s_{n}\right]\in\mathbb{Z}^{n}$, or the
\emph{$\vec{s}$-column degree} of $\mathbf{p}$ is 
\[
\cdeg_{\vec{s}}~\mathbf{p}=\max_{1\le i\le n}[\deg p_{i}+s_{i}]=\deg(x^{\vec{s}}\cdot\mathbf{p}),
\]
where $x^{\vec{s}}=\diag\left(x^{s_{1}},x^{s_{2}},\dots,x^{s_{n}}\right).$
For a matrix $\mathbf{P}$, we use $\cdeg\mathbf{P}$ and $\cdeg_{\vec{s}}\mathbf{P}$
to denote respectively the list of its column degrees and the list
of its shifted $\vec{s}$-column degrees. When $\vec{s}=\left[0,\dots,0\right]$,
the shifted column degree specializes to the standard column degree.
The shifted row degree of a row vector \textbf{$\mathbf{q}=\left[q_{1},\dots,q_{n}\right]$}
is defined similarly as 
\[
\rdeg_{\vec{s}}\mathbf{q}=\max_{1\le i\le n}[\deg q_{i}+s_{i}]=\deg(\mathbf{q}\cdot x^{\vec{s}}).
\]


Shifted degrees have been used previously in polynomial matrix computations
and in generalizations of some matrix normal forms \cite{BLV:jsc06}.


The usefulness of the shifted degrees can be seen from their applications
in polynomial matrix computation problems \cite{ZL2012,za2012}. One
of its uses is illustrated by the following lemma from \cite[Chapter 2]{zhou:phd2012},
which can be viewed as a stronger version of the predictable-degree
property \cite{kailath:1980}. 
\begin{lem}
\label{lem:predictableDegree}Let $\mathbf{A}\in\mathbb{K}\left[x\right]^{m\times n}$
be a $\vec{u}$-column reduced matrix with no zero columns and with
$\cdeg_{\vec{u}}\mathbf{A}=\vec{v}$. Then a matrix $\mathbf{B}\in\mathbb{K}\left[x\right]^{n\times k}$
has $\vec{v}$-column degrees $\cdeg_{\vec{v}}\mathbf{B}=\vec{w}$
if and only if $\cdeg_{\vec{u}}\left(\mathbf{A}\mathbf{B}\right)=\vec{w}$. 
\end{lem}
The following lemma from \cite[Chapter 2]{zhou:phd2012} describes
a relationship between shifted column degrees and shifted row degrees. 
\begin{lem}
\label{lem:columnDegreesRowDegreesSymmetry}A matrix $\mathbf{A}\in\mathbb{K}\left[x\right]^{m\times n}$
has $\vec{u}$-column degrees bounded by $\vec{v}$ if and only if
its $-\vec{v}$-row degrees are bounded by $-\vec{u}$. 
\end{lem}
Another essential fact needed in our algorithm, also based on the
use of the shifted degrees, is the efficient multiplication of matrices
with unbalanced degrees \cite[Theorem 3.7]{za2012}. 
\begin{thm}
\label{thm:multiplyUnbalancedMatrices} Let $\mathbf{A}\in\mathbb{K}\left[x\right]^{m\times n}$
with $m\le n$, $\vec{s}\in\mathbb{Z}^{n}$ a shift with entries bounding
the column degrees of $\mathbf{A}$ and $\xi$, a bound on the sum
of the entries of $\vec{s}$. Let $\mathbf{B}\in\mathbb{K}\left[x\right]^{n\times k}$
with $k\in O\left(m\right)$ and the sum $\theta$ of its $\vec{s}$-column
degrees satisfying $\theta\in O\left(\xi\right)$. Then we can multiply
$\mathbf{A}$ and $\mathbf{B}$ with a cost of $O^{\sim}(n^{2}m^{\omega-2}s)\subset O^{\sim}(n^{\omega}s)$,
where $s=\xi/n$ is the average of the entries of $\vec{s}$. 
\end{thm}

\subsection{Order Basis}

Let $\mathbb{K}$ be a field, $\mathbf{F}\in\mathbb{K}\left[\left[x\right]\right]^{m\times n}$
a matrix of power series and $\vec{\sigma}=\left[\sigma_{1},\dots,\sigma_{m}\right]$
a vector of non-negative integers. 
\begin{defn}
A vector of polynomials $\mathbf{p}\in\mathbb{K}\left[x\right]^{n\times1}$
has \emph{order} $\left(\mathbf{F},\vec{\sigma}\right)$ (or \emph{order}
$\vec{\sigma}$ with respect to $\mathbf{F}$) if $\mathbf{F}\cdot\mathbf{p}\equiv\mathbf{0}\mod x^{\vec{\sigma}}$,
that is, 
\[
\mathbf{F}\cdot\mathbf{p}=x^{\vec{\sigma}}\mathbf{r}
\]
for some $\mathbf{r}\in\mathbb{K}\left[\left[x\right]\right]^{m\times1}$.
If $\vec{\sigma}=\left[\sigma,\dots,\sigma\right]$ has entries uniformly
equal to $\sigma$, then we say that $\mathbf{p}$ has order $\left(\mathbf{F},\sigma\right).$
The set of all order $\left(\mathbf{F},\vec{\sigma}\right)$ vectors
is a free $\mathbb{K}\left[x\right]$-module denoted by $\left\langle \left(\mathbf{F},\vec{\sigma}\right)\right\rangle $. 
\end{defn}
An order basis for $\mathbf{F}$ and $\vec{\sigma}$ is simply a basis
for the $\mathbb{K}\left[x\right]$-module $\left\langle \left(\mathbf{F},\vec{\sigma}\right)\right\rangle $.
We again represent order bases using matrices, whose columns are the
basis elements. We only work with those order bases having minimal
or shifted minimal degrees (also referred to as a reduced order basis
in \cite{BL1997}), that is, their column degrees or shifted column
degrees are the smallest possible among all bases of the module. %A discussion on such minimality and its existence can be found in \cite[Chapter 2]{zhou:phd2012}.


An order basis \cite{BeLa94,BL1997} $\mathbf{P}$ of $\mathbf{F}$
with order $\vec{\sigma}$ and shift $\vec{s}$, or simply an $\left(\mathbf{F},\vec{\sigma},\vec{s}\right)$-basis,
is a basis for the module $\left\langle \left(\mathbf{F},\vec{\sigma}\right)\right\rangle $
having minimal $\vec{s}$-column degrees. If $\vec{\sigma}=\left[\sigma,\dots,\sigma\right]$
is uniform then we simply write $\left(\mathbf{F},\sigma,\vec{s}\right)$-basis.
The precise definition of an $\left(\mathbf{F},\vec{\sigma},\vec{s}\right)$-basis
is as follows. 
\begin{defn}
\label{def:orderBasis}A polynomial matrix $\mathbf{P}$ is an order
basis of $\mathbf{F}$ of order $\vec{\sigma}$ and shift $\vec{s}$,
denoted by $\left(\mathbf{F},\vec{\sigma},\vec{s}\right)$-basis,
if the following properties hold: 
\begin{enumerate}
\item $\mathbf{P}$ is a nonsingular matrix of dimension $n$ and is $\vec{s}$-column
reduced. 
\item $\mathbf{P}$ has order $\left(\mathbf{F},\vec{\sigma}\right)$ (or
equivalently, each column of $\mathbf{P}$ is in $\left\langle (\mathbf{F},\vec{\sigma})\right\rangle $). 
\item Any $\mathbf{q}\in\left\langle \left(\mathbf{F},\vec{\sigma}\right)\right\rangle $
can be expressed as a linear combination of the columns of $\mathbf{P}$,
given by $\mathbf{P}^{-1}\mathbf{q}$. 
\end{enumerate}
\end{defn}
%\begin{comment}
%Note that the module $\left\langle \left(\mathbf{F},\vec{\sigma}\right)\right\rangle $
%does not depend on the shift $\vec{s}$. 
%\end{comment}


In this paper, a $\left(\mathbf{F},\vec{\sigma},\vec{s}\right)$-basis
is also called a $\left(\mathbf{F},\vec{\sigma},\vec{s}\right)$-order
basis to further distinguish it from the kernel basis notation given
in the following subsection. 

Note that any pair of $\left(\mathbf{F},\vec{\sigma},\vec{s}\right)$-order
bases $\mathbf{P}$ and $\mathbf{Q}$ are column bases of each other
and are unimodularly equivalent.

We will need to compute order bases with unbalanced shifts using Algorithm
2 from \cite{za2009}. This computation can be done efficiently as
given by the following result from \cite{za2012}. 
\begin{thm}
\label{thm:unbalancedOrderBasisCost}If %the shift 
$\vec{s}$ satisfies $\vec{s}\le0$ and $-\sum\vec{s}\le m\sigma$,
then a $\left(\mathbf{F},\sigma,\vec{s}\right)$-basis can be computed
with a cost of $O^{\sim}(n^{\omega}d)$ field operations, where $d=m\sigma/n$. 
\end{thm}

\subsection{Kernel Bases}

The kernel of $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$
is the $\mathbb{F}\left[x\right]$-module 
\[
\left\{ \mathbf{p}\in\mathbb{K}\left[x\right]^{n}~|~\mathbf{F}\mathbf{p}=0\right\} 
\]
with a kernel basis of $\mathbf{F}$ being a basis of this module.
Kernel bases are closely related to order bases, as can be seen from
the following definitions. 
\begin{defn}
\label{def:kernelBasis}Given $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$,
a polynomial matrix $\mathbf{N}\in\mathbb{K}\left[x\right]^{n\times*}$
is a (right) kernel basis of $\mathbf{F}$ if the following properties
hold: 
\begin{enumerate}
\item $\mathbf{N}$ is full-rank. 
\item $\mathbf{N}$ satisfies $\mathbf{F}\cdot\mathbf{N}=0$. 
\item Any $\mathbf{q}\in\mathbb{K}\left[x\right]^{n}$ satisfying $\mathbf{F}\mathbf{q}=0$
can be expressed as a linear combination of the columns of $\mathbf{N}$,
that is, there exists some polynomial vector $\mathbf{p}$ such that
$\mathbf{q}=\mathbf{N}\mathbf{p}$. 
\end{enumerate}
\end{defn}
Any pair of kernel bases $\mathbf{N}$ and $\mathbf{M}$ of $\mathbf{F}$
are column bases of each other and are unimodularly equivalent.

A $\vec{s}$-minimal kernel basis of $\mathbf{F}$ is just a kernel
basis that is $\vec{s}$-column reduced. 
\begin{defn}
Given $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$, a polynomial
matrix $\mathbf{N}\in\mathbb{K}\left[x\right]^{n\times*}$ is a $\vec{s}$-minimal
(right) kernel basis of $\mathbf{F}$ if\textbf{ $\mathbf{N}$} is
a kernel basis of $\mathbf{F}$ and $\mathbf{N}$ is $\vec{s}$-column
reduced. We also call a $\vec{s}$-minimal (right) kernel basis of
$\mathbf{F}$ a $\left(\mathbf{F},\vec{s}\right)$-kernel basis.
\end{defn}
In our earlier paper \cite{za2012}, a minimal kernel basis is called
a minimal nullspace basis. In this paper, the term kernel basis is
now used to emphasize the fact that we compute a basis for a $\mathbb{F}\left[x\right]$-module
instead of a basis for a $\mathbb{F}\left(x\right)$-vector space,
since the term nullspace basis usually refers to a basis of some vector
space as in \cite{storjohann-villard:2005}.

We will need to the following result from \cite{za2012} to bound
the sizes of kernel bases.
\begin{thm}
\label{thm:boundOfSumOfShiftedDegreesOfKernelBasis}Suppose $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$
and $\vec{s}\in\mathbb{Z}_{\ge0}^{n}$ is a shift with entries bounding
the corresponding column degrees of $\mathbf{F}$. Then the sum of
the $\vec{s}$-column degrees of any $\vec{s}$-minimal kernel basis
of $\mathbf{F}$ is bounded by $\sum\vec{s}$. 
\end{thm}
We will also need the following result from \cite{za2012} to compute
kernel bases by rows. 
\begin{thm}
\label{thm:continueComputingKernelBasisByRows}Let $\mathbf{G}=\left[\mathbf{G}_{1}^{T},\mathbf{G}_{2}^{T}\right]^{T}\in\mathbb{K}\left[x\right]^{m\times n}$
and $\vec{t}\in\mathbb{Z}^{n}$ a shift vector. If $\mathbf{N}_{1}$
is a $\left(\mathbf{G}_{1},\vec{t}\right)$-kernel basis with $\vec{t}$-column
degrees $\vec{u}$, and $\mathbf{N}_{2}$ is a $\left(\mathbf{G}_{2}\mathbf{N}_{1},\vec{u}\right)$-kernel
basis with $\vec{u}$-column degrees $\vec{v}$, then $\mathbf{N}_{1}\mathbf{N}_{2}$
is a $\left(\mathbf{G},\vec{t}\right)$-kernel basis with $\vec{t}$-column
degrees $\vec{v}$. 
\end{thm}
Also recall the cost of kernel basis computation from \cite{za2012}. 
\begin{thm}
\label{thm:costGeneral} Given an input matrix $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$.
Let $\vec{s}=\cdeg\mathbf{F}$ and $s=\sum\vec{s}/n$ be the average
column degree of $\mathbf{F}$. Then a $\left(\mathbf{F},\vec{s}\right)$-kernel
basis can be computed with a cost of $O^{\sim}\left(n^{\omega}s\right)$
field operations.
\end{thm}

\section{Column Basis via Factorization}

In this section we reduce the problem of determining a column basis
of a polynomial matrix into three separate processes. For this reduction
it turns out to be %Before discussing the efficient computation of column basis, it is 
useful to look at following relationship between column basis, kernel
basis, and unimodular matrices. 
\begin{lem}
\label{lem:unimodular_kernel_columnBasis} Let $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$
and suppose $\mathbf{U}\in\mathbb{K}\left[x\right]^{n\times n}$ is
a unimodular matrix such that $\mathbf{F}\mathbf{U}=\left[0,\mathbf{T}\right]$
with $\mathbf{T}$ of full column rank. Partition $\mathbf{U}=\left[\mathbf{U}_{L},\mathbf{U}_{R}\right]$
such that $\mathbf{F}\cdot\mathbf{U}_{L}=0$ and $\mathbf{F}\mathbf{U}_{R}=\mathbf{T}$.
Then 
\begin{enumerate}
\item $\mathbf{U}_{L}$ is a kernel basis of $\mathbf{F}$ and $\mathbf{T}$
is a column basis of~$\mathbf{F}$. 
\item If $\mathbf{N}$ is any other kernel basis of $\mathbf{F}$, then
$\mathbf{U}^{*}=\left[\mathbf{N},~\mathbf{U}_{R}\right]$ is also
unimodular and also unimodularly transforms $\mathbf{F}$ to $\left[0,\mathbf{T}\right]$. 
\end{enumerate}
\end{lem}
\begin{proof}
Since $\mathbf{F}$ and $\left[0,\mathbf{T}\right]$ are unimodularly
equivalent with $\mathbf{T}$ having full column rank we have that
$\mathbf{T}$ is a column basis of $\mathbf{F}$. It remains to show
that $\mathbf{U}_{L}$ is a kernel basis of $\mathbf{F}$. Since $\mathbf{F}\mathbf{U}_{L}=0$,
$\mathbf{U}_{L}$ is generated by any kernel basis $\mathbf{N}$,
that is, $\mathbf{U}_{L}=\mathbf{N}\mathbf{C}$ for some polynomial
matrix $\mathbf{C}$. Let $r$ be the rank of $\mathbf{F}$, which
is also the column dimension of $\mathbf{T}$ and $\mathbf{U}_{R}$.
Then both $\mathbf{N}$ and $\mathbf{U}_{L}$ have column dimension
$n-r$. Hence $\mathbf{C}$ is a square $(n-r)\times(n-r)$ matrix.
The unimodular matrix $\mathbf{U}$ can be factored as 
\[
\mathbf{U}=\left[\mathbf{N}\mathbf{C},\mathbf{U}_{R}\right]=\left[\mathbf{N},\mathbf{U}_{R}\right]\begin{bmatrix}\mathbf{C} & 0\\
0 & I
\end{bmatrix},
\]
implying that both factors $\left[\mathbf{N},\mathbf{U}_{R}\right]$
and $\begin{bmatrix}\mathbf{C} & 0\\
0 & I
\end{bmatrix}$ are unimodular. Therefore, $\mathbf{C}$ is unimodular and $\mathbf{U}_{L}=\mathbf{N}\mathbf{C}$
is also a kernel basis. Notice that the unimodular matrix $\left[\mathbf{N},\mathbf{U}_{R}\right]$
also transforms $\mathbf{F}$ to $\left[0,\mathbf{T}\right]$. \end{proof}
\begin{rem}
\label{cor:unimodular_kernel_columnBasis2} It is interesting to see
what Lemma \ref{lem:unimodular_kernel_columnBasis} implies in the
case of unimodular matrices. Let $\mathbf{U}\in\mathbb{K}\left[x\right]^{n\times n}$
be a unimodular matrix with inverse $\mathbf{V}$, which, for a given
$k$, are partitioned as $\mathbf{U}=\left[\mathbf{U}_{L},\mathbf{U}_{R}\right]$
and $\mathbf{V}=\begin{bmatrix}\mathbf{V}_{U}\\
\mathbf{V}_{D}
\end{bmatrix}$ with $\mathbf{U}_{L}\in\mathbb{K}\left[x\right]^{n\times k}$ and
$\mathbf{V}_{U}\in\mathbb{K}\left[x\right]^{k\times n}$. Since $\mathbf{U}$
and $\mathbf{V}$ are inverses of each other we have the identities
\begin{equation}
\mathbf{V}\mathbf{U}=\begin{bmatrix}\mathbf{V}_{U}\mathbf{U}_{L} & \mathbf{V}_{U}\mathbf{U}_{R}\\
\mathbf{V}_{D}\mathbf{U}_{L} & \mathbf{V}_{D}\mathbf{U}_{R}
\end{bmatrix}=\begin{bmatrix}I_{k} & 0\\
0 & I_{n-k}
\end{bmatrix}.\label{inverse}
\end{equation}
Lemma \ref{lem:unimodular_kernel_columnBasis} then gives: 
\begin{enumerate}
\item $I_{k}$ is a column basis of $\mathbf{V}_{U}$ and a row basis of
$\mathbf{U}_{L}$, 
\item $I_{n-k}$ is a column basis of $\mathbf{V}_{D}$ and a row basis
of $\mathbf{U}_{R}$, 
\item $\mathbf{V}_{D}$ and $\mathbf{U}_{L}$ are kernel bases of each other, 
\item $\mathbf{V}_{U}$ and $\mathbf{U}_{R}$ are kernel bases of each other. 
\end{enumerate}
\end{rem}
\begin{lem}
\label{lem:matrixGCD} Let $\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$
with rank $r$. Suppose %\begin{itemize}
%\item[(i)]
 $\mathbf{N}\in\mathbb{K}\left[x\right]^{n\times(n-r)}$ is a right
kernel basis of $\mathbf{F}$ and % \item[(ii)]
$\mathbf{G}\in\mathbb{K}\left[x\right]^{r\times n}$ is a left kernel
basis of $\mathbf{N}$. %\end{itemize}
Then $\mathbf{F}=\mathbf{T}\cdot\mathbf{G}$ with $\mathbf{T}\in\mathbb{K}\left[x\right]^{m\times r}$
a column basis of $\mathbf{F}$. \end{lem}
\begin{proof}
Let $\mathbf{U}=\begin{bmatrix}\mathbf{U}_{L},\mathbf{U}_{R}\end{bmatrix}$
be a unimodular matrix with inverse $\mathbf{V}=\begin{bmatrix}\mathbf{V}_{U}\\
\mathbf{V}_{D}
\end{bmatrix}$ partitioned as in equation (\ref{inverse}) and satisfying $\mathbf{F}\cdot\mathbf{U}=\left[0,\mathbf{B}\right]$
with $\mathbf{B}\in\mathbb{K}\left[x\right]^{m\times r}$ a column
basis of $\mathbf{F}$. Then %\textbf{ 
$\mathbf{F}=\left[0,\mathbf{B}\right]\mathbf{U}^{-1}=\mathbf{B}\left[0,I\right]\mathbf{V}=\mathbf{B}\mathbf{V}_{D}.$
%}
 Since $\mathbf{V}_{D}$ is a left kernel basis of\textbf{ $\mathbf{U}_{L}$},
any other left kernel basis $\mathbf{G}$ of $\mathbf{U}_{L}$ is
unimodularly equivalent to $\mathbf{V}_{D}$, that is, $\mathbf{V}_{D}=\mathbf{W}\cdot\mathbf{G}$
for some unimodular matrix $\mathbf{W}$. Thus $\mathbf{F}=\mathbf{B\cdot W}\cdot\mathbf{G}$.
Then $\mathbf{T}=\mathbf{B\cdot W}$ is a column basis of $\mathbf{F}$
since it is unimodularly equivalent to the column basis $\mathbf{B}$. 
\end{proof}
Lemma \ref{lem:matrixGCD} outlines a procedure for computing a column
basis of $\mathbf{F}$ with three main steps. The first step is to
compute a right kernel basis $\mathbf{N}$ of $\mathbf{F}$, something
which can be efficiently done using the kernel basis algorithm of
\cite{za2012}. The second step, computing a left kernel basis $\mathbf{G}$
for $\mathbf{N}$ and the third step, computing the column basis $\mathbf{T}$
from $\mathbf{F}$ and $\mathbf{G}$, will still require additional
work for efficient computation. Note that, while Lemma \ref{lem:matrixGCD}
does not require the bases computed to be minimal, working with minimal
kernel bases keeps the degrees well controlled, an important consideration
for efficient computation. 
\begin{exmp}
Let 
\[
\mathbf{F}=\left[\begin{array}{cccc}
x^{2} & x^{2} & x+x^{2} & 1+x^{2}\\
1+x+x^{2} & x^{2} & 1+x^{2} & 1+x^{2}
\end{array}\right]~
\]
be a matrix over $\mathbb{Z}_{2}[x]$. Then the matrix 
\[
\mathbf{N}=\left[\begin{array}{cc}
x & 1\\
1 & x\\
x & 1\\
0 & x
\end{array}\right]
\]
is a right kernel basis of $\mathbf{F}$ and the matrix 
\[
\mathbf{G}=\left[\begin{array}{cccc}
1 & 0 & 1 & 0\\
\noalign{\medskip}x & {x}^{2} & 0 & 1+{x}^{2}
\end{array}\right]
\]
is a left kernel basis of $\mathbf{N}$. Finally the matrix 
\[
\mathbf{T}=\left[\begin{array}{cc}
x+x^{2} & 1\\
1+x^{2} & 1
\end{array}\right]
\]
satisfies $\mathbf{F}=\mathbf{T}\mathbf{G}$, and is a column basis
of $\mathbf{F}$. \qed 
\end{exmp}

\section{\label{sec:computeRightFactor}Computing a Right Factor}

Let $\mathbf{N}$ be an $\left(\mathbf{F},\vec{s}\right)$-kernel
basis computed using the existing algorithm from \cite{za2012}. Consider
now the problem of computing a left $-\vec{s}$-minimal kernel basis
$\mathbf{G}$ for $\mathbf{N}$, or equivalently, a right $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis $\mathbf{G}^{T}$. For this problem, the kernel basis algorithm
of \cite{za2012} cannot be applied directly, since the input matrix
$\mathbf{N}^{T}$ has nonuniform row degrees and negative shift. Comparing
to the earlier problem of computing a $\vec{s}$-minimal kernel basis
$\mathbf{N}$ for $\mathbf{F}$, it is interesting to note that the
original output $\mathbf{N}$ now becomes the new input matrix $\mathbf{N}^{T}$,
while the new output matrix $\mathbf{G}$ has size bounded by the
size of $\mathbf{F}$. In other words, the new input has degrees that
match the original output, while the new output has degrees bounded
by the original input. It is therefore reasonable to expect that the
new problem can be computed efficiently. However, we need to find
some way to work with the more complicated input degree structure.
On the other hand, the simpler output degree structure makes it easier
to apply order basis computation in order to compute a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis.


\subsection{Kernel Bases via Order Bases}

In order to see how order basis computations can be applied here,
let us first recall the following result (Lemma 3.3 \cite{za2012})
on a relationship between order bases and kernel bases. 
\begin{lem}
\label{lem:orderBasisContainsNullspaceBasis}Let $\mathbf{P}=\left[\mathbf{P}_{L},\mathbf{P}_{R}\right]$
be any $\left(\mathbf{F},\sigma,\vec{s}\right)$-order basis and $\mathbf{N}=\left[\mathbf{N}_{L},\mathbf{N}_{R}\right]$
be any $\vec{s}$-minimal kernel basis of $\mathbf{F}$, where $\mathbf{P}_{L}$
and $\mathbf{N}_{L}$ contain all columns from $\mathbf{P}$ and $\mathbf{N}$,
respectively, whose $\vec{s}$-column degrees are less than $\sigma$.
Then $\left[\mathbf{P}_{L},\mathbf{N}_{R}\right]$ is a $\vec{s}$-minimal
kernel basis of $\mathbf{F}$, and $\left[\mathbf{N}_{L},\mathbf{P}_{R}\right]$
is a $\left(\mathbf{F},\sigma,\vec{s}\right)$-order basis. 
\end{lem}
It is not difficult to extend this result to the following lemma to
accommodate our situation here.%
\begin{comment}
For the remainder of this paper an integer vector of ones is denoted
by $\vec{e}$. 
\end{comment}

\begin{lem}
\label{lem:orderbasisContainsKernelbasisGeneralized} Given a matrix
$\mathbf{A}\in\mathbb{K}\left[x\right]^{m\times n}$ and some integer
lists $\vec{u}\in\mathbb{Z}^{n}$ and $\vec{v}\in\mathbb{Z}^{m}$
such that $\rdeg_{\vec{u}}\mathbf{A}\le\vec{v}$, or equivalently,
$\cdeg_{-\vec{v}}\mathbf{A}\le-\vec{u}$. Let $\mathbf{P}$ be a $\left(\mathbf{A},\vec{v}+1,-\vec{u}\right)$-order
basis and $\mathbf{Q}$ be any $(\mathbf{A},-\vec{u})$-kernel basis.
Partition $\mathbf{P}=\left[\mathbf{P}_{L},\mathbf{P}_{R}\right]$
and $\mathbf{Q}=\left[\mathbf{Q}_{L},\mathbf{Q}_{R}\right]$ where
$\mathbf{P}_{L}$ and $\mathbf{Q}_{L}$ contain all the columns from
$\mathbf{P}$ and $\mathbf{Q}$, respectively, whose $-\vec{u}$-column
degrees are no more than $0$. Then 
\begin{itemize}
\item [(i)] $\left[\mathbf{P}_{L},\mathbf{Q}_{R}\right]$ is an $(\mathbf{A},-\vec{u})$-kernel
basis, and 
\item [(ii)] $\left[\mathbf{Q}_{L},\mathbf{P}_{R}\right]$ is an $\left(\mathbf{A},\vec{v}+1,-\vec{u}\right)$-order
basis. 
\end{itemize}
\end{lem}
\begin{proof}
We can use the same proof from Lemma 3.3 in \cite{za2012}. We know
$\cdeg_{-\vec{v}}\mathbf{A}\mathbf{P}_{L}\le\cdeg_{-\vec{u}}\mathbf{P}_{L}\le0$,
or equivalently, $\rdeg\mathbf{A}\mathbf{P}_{L}\le\vec{v}$. However
it also has order greater than $\vec{v}$ and hence $\mathbf{A}\mathbf{P}_{L}=0$.
Thus $\mathbf{P}_{L}$ is generated by the kernel basis $\mathbf{Q}_{L}$,
that is, $\mathbf{P}_{L}=\mathbf{Q}_{L}\mathbf{U}$ for some polynomial
matrix $\mathbf{U}$. On the other hand, $\mathbf{Q}_{L}$ certainly
has order $\left(\mathbf{A},\vec{v}+1\right)$ and therefore is generated
by $\mathbf{P}_{L}$, that is, $\mathbf{Q}_{L}=\mathbf{P}_{L}\mathbf{V}$
for some polynomial matrix $\mathbf{V}$. We now have $\mathbf{P}_{L}=\mathbf{P}_{L}\mathbf{V}\mathbf{U}$
and $\mathbf{Q}_{L}=\mathbf{Q}_{L}\mathbf{U}\mathbf{V}$, implying
both $\mathbf{U}$ and $\mathbf{V}$ are unimodular. The result then
follows from the unimodular equivalence of $\mathbf{P}_{L}$ and $\mathbf{Q}_{L}$
and the fact that they are $-\vec{u}$-column reduced. 
\end{proof}
With the help of Lemma \ref{lem:orderbasisContainsKernelbasisGeneralized}
we can return to the problem of efficiently computing a $(\mathbf{N}^{T},-\vec{s})$-kernel
basis. In fact, we just need to use a special case of Lemma \ref{lem:orderbasisContainsKernelbasisGeneralized},
where all the elements of the kernel basis have shifted degrees bounded
by $0$, thereby making the partial kernel basis be a complete kernel
basis. 
\begin{lem}
\label{lem:kernelBasisInOrderBasis} Let $\mathbf{N}$ be a $(\mathbf{F},\vec{s})$-kernel
basis with $\cdeg_{\vec{s}}~\mathbf{N}=\vec{b}$. Then any $(\mathbf{N}^{T},-\vec{s})$-kernel
basis $\mathbf{G}^{T}$ satisfies $\cdeg_{-\vec{s}}\mathbf{G}^{T}\le0$.
Let $\mathbf{P}=\left[\mathbf{P}_{L},\mathbf{P}_{R}\right]$ be a
$\left(\mathbf{N}^{T},\vec{b}+1,-\vec{s}\right)$-order basis, where
$\mathbf{P}_{L}$ consists of all columns $\mathbf{p}$ satisfying
$\cdeg_{-\vec{s}}~\mathbf{p}\le0$. Then $\mathbf{P}_{L}$ is a $(\mathbf{N}^{T},-\vec{s})$-kernel
basis. \end{lem}
\begin{proof}
The column dimension of any $(\mathbf{N}^{T},-\vec{s})$-kernel basis
$\mathbf{G}^{T}$ equals the rank $r$ of $\mathbf{F}$. Since both
$\mathbf{F}$ and $\mathbf{G}$ are in the left kernel of $\mathbf{N}$,
we know $\mathbf{F}$ is generated by $\mathbf{G}$, and the $-\vec{s}$-minimality
of $\mathbf{G}$ ensures that the $-\vec{s}$-row degrees of $\mathbf{G}$
are bounded by the corresponding $r$ largest $-\vec{s}$-row degrees
of $\mathbf{F}$. These are in turn bounded by $0$ since $\cdeg\mathbf{F}\le\vec{s}$.
Therefore, any $(\mathbf{N}^{T},-\vec{s})$-kernel basis $\mathbf{G}^{T}$
satisfies $\cdeg_{-\vec{s}}\mathbf{G}^{T}\le0$. The result follows
from Lemma \ref{lem:orderbasisContainsKernelbasisGeneralized}. 
\end{proof}
While \prettyref{lem:kernelBasisInOrderBasis} shows that a complete
$(\mathbf{N}^{T},-\vec{s})$-kernel basis can be computed by computing
a $\left(\mathbf{N}^{T},\vec{b}+1,-\vec{s}\right)$-order basis, in
fact we do not compute such a order basis, as the computational efficiency
can be improved by using Theorem \ref{thm:continueComputingKernelBasisByRows}
to compute a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis
by rows. More specifically, we can partition $\mathbf{N}$ into $\left[\mathbf{N}_{1},\mathbf{N}_{2}\right]$
with $\vec{s}$-column degrees $\vec{b}_{1}$, $\vec{b}_{2}$ respectively,
compute a $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel basis
$\mathbf{Q}_{1}$ with $-\vec{s}$-column degrees $-\vec{s}_{2}$,
and then compute a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},-\vec{s}_{2}\right)$-kernel
basis $\mathbf{Q}_{2}$, then $\mathbf{Q}_{1}\mathbf{Q}_{2}$ is a
$\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis. In order to
compute the kernel bases $\mathbf{Q}_{1}$ and $\mathbf{Q}_{2}$,
we still use order basis computations but work with subsets of rows
rather than the whole matrix $\mathbf{N}^{T}$. We now need to make
sure that the order bases computed from subsets of rows contain these
kernel bases. 
\begin{lem}
\label{lem:kernelBasisOfSubsetOfRowsContainedInOrderBasis} Let $\mathbf{N}$
be partitioned as $\left[\mathbf{N}_{1},\mathbf{N}_{2}\right]$, with
$\vec{s}$-column degrees $\vec{b}_{1}$, $\vec{b}_{2}$, respectively.
Then we have the following: 
\begin{enumerate}
\item A $\left(\mathbf{N}_{1}^{T},\vec{b}_{1}+1,-\vec{s}\right)$-order
basis contains a $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel
basis whose $-\vec{s}$-column degrees are bounded by $0$. 
\item If $\mathbf{Q}_{1}$ is this $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel
basis from above and $-\vec{s}_{2}=\cdeg_{-\vec{s}}\mathbf{Q}_{1}$,
then a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},\vec{b}_{2}+1,-\vec{s}_{2}\right)$-basis
contains a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},-\vec{s}_{2}\right)$-kernel
basis, $\mathbf{Q}_{2}$, whose $-\vec{s}$-column degrees are bounded
by $0$. 
\item The product $\mathbf{Q}_{1}\mathbf{Q}_{2}$ is a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis. 
\end{enumerate}
\end{lem}
\begin{proof}
To see that a $\left(\mathbf{N}_{1}^{T},\vec{b}_{1}+1,-\vec{s}\right)$-basis
contains a $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel basis
whose $-\vec{s}$-column degrees are bounded by 0, we just need to
show that $\cdeg_{-\vec{s}}\mathbf{\bar{Q}}_{1}\le0$ for any $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel
basis $\mathbf{\bar{Q}}_{1}$ and then apply \prettyref{lem:orderbasisContainsKernelbasisGeneralized}.
Note that there exists a polynomial matrix $\bar{\mathbf{Q}}_{2}$
such that $\mathbf{\bar{Q}}_{1}\mathbf{\bar{Q}}_{2}=\bar{\mathbf{G}}$
for any $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis $\bar{\mathbf{G}}$,
as $\bar{\mathbf{G}}$ satisfies $\mathbf{N}_{1}^{T}\bar{\mathbf{G}}=0$
and is therefore generated by the $\left(\mathbf{N}_{1}^{T},-\vec{s}\right)$-kernel
basis $\bar{\mathbf{Q}}_{1}$. If $\cdeg_{-\vec{s}}\mathbf{\bar{Q}}_{1}\nleq0$,
then \prettyref{lem:predictableDegree} forces $\cdeg_{-\vec{s}}\left(\bar{\mathbf{Q}}_{1}\bar{\mathbf{Q}}_{2}\right)=\cdeg_{-\vec{s}}\bar{\mathbf{G}}\nleq0,$a
contradiction since we know from \prettyref{lem:kernelBasisInOrderBasis}
that $\cdeg_{-\vec{s}}\bar{\mathbf{G}}\le0$.

As before, to see that a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},\vec{b}_{2}+1,-\vec{s}_{2}\right)$-basis
contains a $\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},-\vec{s}_{2}\right)$-kernel
basis whose $-\vec{s}$-column degrees are no more than 0, we can
just show $\cdeg_{-\vec{s}_{2}}\hat{\mathbf{Q}}_{2}\le0$ for any
$\left(\mathbf{N}_{2}^{T}\mathbf{Q}_{1},-\vec{s}_{2}\right)$-kernel
basis $\hat{\mathbf{Q}}_{2}$ and then apply \prettyref{lem:orderbasisContainsKernelbasisGeneralized}.
Since $\cdeg_{\vec{s}}\mathbf{N}_{2}=\vec{b}_{2}$, we have $\rdeg_{-\vec{b}_{2}}\mathbf{N}_{2}\le-\vec{s}$
or equivalently, $\cdeg_{-\vec{b}_{2}}\mathbf{N}_{2}^{T}\le-\vec{s}.$
Then combining this with $\cdeg_{-\vec{s}}\mathbf{Q}_{1}=-\vec{s}_{2}$
we get $\cdeg_{-\vec{b}_{2}}\mathbf{N}_{2}^{T}\mathbf{Q}_{1}\le-\vec{s}_{2}$
using \prettyref{lem:predictableDegree}. Let $\hat{\mathbf{G}}=\mathbf{Q}_{1}\hat{\mathbf{Q}}_{2}$,
which is a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis by
\prettyref{thm:continueComputingKernelBasisByRows}. Note that $\cdeg_{-\vec{s}_{2}}\hat{\mathbf{Q}}_{2}=\cdeg_{-\vec{s}}\mathbf{Q}_{1}\hat{\mathbf{Q}}_{2}=\cdeg_{-\vec{s}}\hat{\mathbf{G}}\le0.$ 
\end{proof}

\subsection{\label{sub:kernelBasisViaOrderBasisByRows}Efficient Computation
of Kernel Bases}

Now that we can correctly compute a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis by rows with the help of order basis computation using \prettyref{lem:kernelBasisOfSubsetOfRowsContainedInOrderBasis},
we need to look at how to do this efficiently. One major difficulty
is that the order $\vec{b}+1$, or equivalently, the $\vec{s}$-row
degrees of $\mathbf{N}^{T}$ may be unbalanced and can have degree
as large as $\sum\vec{s}$. Note that the existing kernel basis algorithm
from \cite{za2012} handles input matrices with unbalanced column
degrees, but not unbalanced row degrees. For example, in the simpler
special case of $\vec{s}=\left[s,\dots,s\right]$ having uniformly
equal entries, the sum of the row degrees is $O(ns)$, but the sum
of column degrees can be $\Theta\left(n^{2}s\right)$, which puts
an extra factor $n$ to the cost if the algorithm from \cite{za2012}
is used. To overcome this problem with unbalanced $\vec{s}$-row degrees,
we separate the rows of $\mathbf{N}^{T}$ into blocks according to
their $\vec{s}$-row degrees, and then work with these blocks one
by one successively using \prettyref{lem:kernelBasisOfSubsetOfRowsContainedInOrderBasis}.

\begin{algorithm}[t]
\caption{$\mnbr(\mathbf{M},\vec{s},\xi)$}


(Kernel basis computation with reversed degree structure)

\label{alg:minimalKernelBasisReverse}

\smallskip{}
\begin{algorithmic}[1]
\REQUIRE{$\mathbf{M}\in\mathbb{K}\left[x\right]^{k\times n}$ and $\vec{s}\in\mathbb{Z}_{\ge0}^{n}$
such that $\sum\rdeg_{\vec{s}}\mathbf{M}\le\xi$, $\sum\vec{s}\le\xi$,
and any $\left(\mathbf{M},-\vec{s}\right)$-kernel basis having row
degrees bounded by $\vec{s}$ (equivalently, having $-\vec{s}$-column
degrees bounded by 0).}

\begin{comment}
\bigskip{}
\end{comment}


\ENSURE{$\mathbf{G}\in\mathbb{K}\left[x\right]^{n\times*}$, a $\left(\mathbf{M},-\vec{s}\right)$-kernel
basis.}

\smallskip{}


\STATE{\textbf{$\left[\mathbf{M}_{1}^{T},\mathbf{M}_{2}^{T},\cdots,\mathbf{M}_{\left\lceil \log k\right\rceil -1}^{T},\mathbf{M}_{\left\lceil \log k\right\rceil }^{T}\right]:=\mathbf{M}^{T}$},
with $\mathbf{M}_{\left\lceil \log k\right\rceil },\mathbf{M}_{\left\lceil \log k\right\rceil -1},\cdots,\mathbf{M}_{2},\mathbf{M}_{1}$
having $\vec{s}$-row degrees in the range $\left[0,\frac{2\xi}{k}\right],(\frac{2\xi}{k},\frac{4\xi}{k}],...,(\frac{\xi}{4},\frac{\xi}{2}],(\frac{\xi}{2},\xi].$\textbf{ }}

\FOR{$i$ \textbf{from $1$ to $\left\lceil \log k\right\rceil $ }} 

\forbody{\STATE{$\sigma_{i}:=\left\lceil \frac{\xi}{2^{i-1}}\right\rceil +1$;$\vec{\sigma}_{i}:=\left[\sigma_{i},\dots,\sigma_{i}\right]$,
number of entries matching the row dimension of $\mathbf{M}_{i};$}}

\STATE{$\vec{\sigma}:=\left[\vec{\sigma}_{1},\vec{\sigma}_{2},\dots,\vec{\sigma}_{\left\lceil \log k\right\rceil }\right]$;}

\STATE{$\hat{\mathbf{N}}:=x^{\vec{\sigma}-\vec{b}-1}\mathbf{M};$}

\STATE{$\mathbf{G}_{0}:=I_{n}$; $\tilde{\mathbf{G}}_{0}:=I_{n};$}

\FOR{$i$ \textbf{from $1$ to $\left\lceil \log k\right\rceil $ }} 

\forbody{\STATE{$\vec{s}_{i}:=-\cdeg_{-\vec{s}}\mathbf{G}_{i-1};$ (note $\vec{s}_{1}=\vec{s}$)}

\STATE{$\mathbf{P}_{i}:=\umab\left(\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1},\sigma_{i},-\vec{s}_{i}\right)$; }

\STATE{$\left[\mathbf{G}_{i},\mathbf{Q}_{i}\right]:=\mathbf{P}_{i}$, where
$\mathbf{G}_{i}$ is a $\left(\hat{\mathbf{M}}_{i},-\vec{s}_{i}\right)$-kernel
basis;}

\STATE{$\tilde{\mathbf{G}_{i}}:=\tilde{\mathbf{G}}_{i-1}\cdot\mathbf{G}_{i};$ }

}



\RETURN $\tilde{\mathbf{G}_{i}}$ 
\end{algorithmic}
\end{algorithm}


Let $k$ be the column dimension of $\mathbf{N}$ and $\xi$ be an
upper bound of $\sum\vec{s}$. Since $\sum\cdeg_{\vec{s}}\mathbf{N}=\sum\vec{b}\le\sum\vec{s}\le\xi$
by \prettyref{thm:boundOfSumOfShiftedDegreesOfKernelBasis}, at most
$\frac{k}{c}$ columns of $\mathbf{N}$ have $\vec{s}$-column degrees
greater than or equal to $\frac{c~\xi}{k}$ for any $c\ge1$. Without
loss of generality we can assume that the rows of $\mathbf{N}^{T}$
are arranged in decreasing $\vec{s}$-row degrees. We divide $\mathbf{N}^{T}$
into $\left\lceil \log k\right\rceil $ row blocks according to the
$\vec{s}$-row degrees of its rows, or equivalently, divide $\mathbf{N}$
into blocks of columns according to the $\vec{s}$-column degrees.
Let $\mathbf{N}=\left[\mathbf{N}_{1},\mathbf{N}_{2},\cdots,\mathbf{N}_{\left\lceil \log k\right\rceil -1},\mathbf{N}_{\left\lceil \log k\right\rceil }\right]$
with $\mathbf{N}_{\left\lceil \log k\right\rceil },\mathbf{N}_{\left\lceil \log k\right\rceil -1},\dots,\mathbf{N}_{2},\mathbf{N}_{1}$
having $\vec{s}$-column degrees in the range $\left[0,2\xi/k\right]$,
$(2\xi/k,4\xi/k],$ $(4\xi/k,8\xi/k],\ ...,$ $(\xi/4,\xi/2],$ $(\xi/2,\xi]$,
respectively. Let $\sigma_{i}=\left\lceil \xi/2^{i-1}\right\rceil +1$
and $\vec{\sigma}_{i}=\left[\sigma_{i},\dots,\sigma_{i}\right]$ with
the same dimension as the row dimension of $\mathbf{N}_{i}$ and $\vec{\sigma}=\left[\vec{\sigma}_{\left\lceil \log k\right\rceil },\vec{\sigma}_{\left\lceil \log k\right\rceil -1},\dots,\vec{\sigma}_{1}\right]$
be the orders in the order basis computation.

To further simplify our task, we also make the order of our problem
in each block uniform. Rather than of using $\mathbf{N}^{T}$ as the
input matrix, we instead use 
\begin{eqnarray*}
\hat{\mathbf{N}} & =\begin{bmatrix}\hat{\mathbf{N}}_{1}\\
\vdots\\
\hat{\mathbf{N}}_{\left\lceil \log k\right\rceil }
\end{bmatrix}= & x^{\vec{\sigma}-\vec{b}-1}\begin{bmatrix}\mathbf{N}_{1}^{T}\\
\vdots\\
\mathbf{N}_{\left\lceil \log k\right\rceil }^{T}
\end{bmatrix}=x^{\vec{\sigma}-\vec{b}-1}\mathbf{N}^{T}
\end{eqnarray*}
so that a $\left(\hat{\mathbf{N}},\vec{\sigma},-\vec{s}\right)$-order
basis is a $\left(\mathbf{N}^{T},\vec{b}+1,-\vec{s}\right)$-order
basis.

In order to compute a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis we determine a series of kernel bases via a series of order
basis computations as follows: 
\begin{enumerate}
\item Let $\vec{s}_{1}=\vec{s}$. Compute an $\left(\hat{\mathbf{N}}_{1},\vec{\sigma}_{1},-\vec{s}_{1}\right)$-order
basis $\mathbf{P}_{1}$ using Algorithm 2 from \cite{ZL2012} for
order basis computation with unbalanced shift. Note that here the
order $\vec{\sigma}_{1}=\left[\sigma_{1},\dots,\sigma_{1}\right]$
is uniform, an $\left(\hat{\mathbf{N}}_{1},\vec{\sigma}_{1},-\vec{s}_{1}\right)$
order basis is also $\left(\hat{\mathbf{N}}_{1},\sigma_{1},-\vec{s}_{1}\right)$-order
basis. Partition $\mathbf{P}_{1}$ as $\mathbf{P}_{1}=\left[\mathbf{G}_{1},\mathbf{Q}_{1}\right]$,
where $\mathbf{G}_{1}$ is a $\left(\hat{\mathbf{N}}_{1},-\vec{s}_{1}\right)$-kernel
basis by \prettyref{lem:kernelBasisOfSubsetOfRowsContainedInOrderBasis}.
Set $\tilde{\mathbf{G}}_{1}=\mathbf{G}_{1}$ and $\vec{s}_{2}=-\cdeg_{-\vec{s}}\mathbf{G}_{1}$. 
\item Compute an $\left(\hat{\mathbf{N}}_{2}\tilde{\mathbf{G}}_{1},\sigma_{2},-\vec{s}_{2}\right)$-order
basis $\mathbf{P}_{2}$ and partition $\mathbf{P}_{2}=\left[\mathbf{G}_{2},\mathbf{Q}_{2}\right]$
with $\mathbf{G}_{2}$ a $\left(\hat{\mathbf{N}}_{2},-\vec{s}_{2}\right)$-kernel
basis. Set $\vec{s}_{3}=-\cdeg_{-\vec{s}_{2}}\mathbf{G}_{2}$ and
$\tilde{\mathbf{G}}_{2}=\tilde{\mathbf{G}}_{1}\mathbf{G}_{2}$. 
\item Continuing this process, at each step $i$ we compute a $\left(\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1},\sigma_{i},-\vec{s}_{i}\right)$-order
basis $\mathbf{P}_{i}$ and then partition $\mathbf{P}_{i}=\left[\mathbf{G}_{i},\mathbf{Q}_{i}\right]$
with $\mathbf{G}_{i}$ a $\left(\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1},-\vec{s}_{i}\right)$-kernel
basis. Let $\tilde{\mathbf{G}}_{i}=\prod_{j=1}^{i}\mathbf{G}_{i}=\tilde{\mathbf{G}}_{i-1}\mathbf{G}_{i}$. 
\item Return $\tilde{\mathbf{G}}_{\left\lceil \log k\right\rceil }$, a
$\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel basis. 
\end{enumerate}
This process of computing a $\left(\mathbf{N}^{T},-\vec{s}\right)$-kernel
basis is formally given in Algorithm \ref{alg:minimalKernelBasisReverse}.


\subsection{Complexity of Left Kernel Basis Computation}

The cost of Algorithm \ref{alg:minimalKernelBasisReverse} is dominated
by the order basis computations and the multiplications $\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1}$
and $\tilde{\mathbf{G}}_{i-1}\mathbf{G}_{i}$. Let $s=\xi/n$. 
\begin{lem}
An $\left(\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1},\sigma_{i},-\vec{s}_{i}\right)$-order
basis can be computed with a cost of $O^{\sim}\left(n^{\omega}s\right)$. \end{lem}
\begin{proof}
Note that $\mathbf{N}_{i}$ has less than $2^{i}$ columns. Otherwise,
since $\cdeg_{\vec{s}}\mathbf{N}_{i}>\xi/2^{i}$, we have $\sum\cdeg_{\vec{s}}\mathbf{N}_{i}>2^{i}\xi/2^{i}=\xi,$
contradicting with $\sum\cdeg_{\vec{s}}\mathbf{N}=\sum\vec{b}\le\sum\vec{s}\le\xi.$
It follows that $\hat{\mathbf{N}}_{i}$, and therefore $\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1}$,
also have less than $2^{i}$ rows. We also have $\sigma_{i}=\left\lceil \xi/2^{i-1}\right\rceil +1\in\Theta\left(\xi/2^{i}\right)$.
Therefore, Algorithm 2 from \cite{ZL2012} for order basis computation
with unbalanced shift can be used with a cost of $O^{\sim}\left(n^{\omega}s\right)$. \end{proof}
\begin{lem}
The multiplications $\hat{\mathbf{N}}_{i}\tilde{\mathbf{G}}_{i-1}$
can be done with a cost of $O^{\sim}\left(n^{\omega}s\right)$.\end{lem}
\begin{proof}
The dimension of $\hat{\mathbf{N}}_{i}$ is bounded by $2^{i}\times n$
and $\sum\rdeg_{\vec{s}}\hat{\mathbf{N}}_{i}\le2^{i}\cdot\xi/2^{i-1}\in O\left(\xi\right)$.
We also have $\cdeg_{-\vec{s}}\tilde{\mathbf{G}}_{i-1}\le0$, or equivalently,
$\rdeg\tilde{\mathbf{G}}_{i-1}\le\vec{s}$. We can now use Theorem
\ref{thm:multiplyUnbalancedMatrices} to multiply $\tilde{\mathbf{G}}_{i-1}^{T}$
and $\hat{\mathbf{N}}_{i}^{T}$ with a cost of $O^{\sim}\left(n^{\omega}s\right)$.\end{proof}
\begin{lem}
The multiplication $\tilde{\mathbf{G}}_{i-1}\mathbf{G}_{i}$ can be
done with a cost of $O^{\sim}\left(n^{\omega}s\right)$. \end{lem}
\begin{proof}
We know $\cdeg_{-\vec{s}}\tilde{\mathbf{G}}_{i-1}=-\vec{s}_{i}$,
and $\cdeg_{-\vec{s}_{i}}\mathbf{G}_{i}=-\vec{s}_{i+1}\le0.$ In other
words, $\rdeg\mathbf{G}_{i}\le\vec{s}_{i}$, and $\rdeg_{\vec{s}_{i}}\tilde{\mathbf{G}}_{i-1}\le\vec{s}$,
hence we can again use Theorem \ref{thm:multiplyUnbalancedMatrices}
to multiply $\mathbf{G}_{i}^{T}$ and $\tilde{\mathbf{G}}_{i-1}^{T}$
with a cost of $O^{\sim}\left(n^{\omega}s\right)$. \end{proof}
\begin{lem}
\label{lem:costKernelBasisReverse}Given an input matrix $\mathbf{M}\in\mathbb{K}\left[x\right]^{k\times n}$,
a shift $\vec{s}\in\mathbb{Z}^{n}$, and an upper bound $\xi\in\mathbb{Z}$
such that 
\begin{itemize}
\item [(i)] $\sum\rdeg_{\vec{s}}\mathbf{M}\le\xi$, 
\item [(ii)] $\sum\vec{s}\le\xi$, 
\item [(iii)] any $\left(\mathbf{M},-\vec{s}\right)$-kernel basis having
row degrees bounded by $\vec{s}$, or equivalently, $-\vec{s}$-column
degrees bounded by 0. 
\end{itemize}
Then \prettyref{alg:minimalKernelBasisReverse} costs $O^{\sim}\left(n^{\omega}s\right)$
field operations to compute a $\left(\mathbf{M},-\vec{s}\right)$-kernel
basis. 
\end{lem}
Note that while the upper bound $\xi$ can be simply replaced by $\sum\vec{s}$
in Lemma \ref{lem:costKernelBasisReverse} and \prettyref{alg:minimalKernelBasisReverse}
for computing a right factor in this section, keeping it separate
makes the algorithm more general and allows it to be reused in the
next section. 

It may also be informative to note again the correspondence between
\prettyref{lem:costKernelBasisReverse} and \prettyref{thm:costGeneral},
on the reversal of the degree structures of the input matrices and
the output kernel bases.
\begin{thm}
A right factor $\mathbf{G}$ satisfying $\mathbf{F}=\mathbf{TG}$
for a column basis $\mathbf{T}$ can be computed with a cost of $O^{\sim}\left(n^{\omega}s\right)$. 
\end{thm}

\section{Computing a Column Basis}

Once a right factor $\mathbf{G}$ of $\mathbf{F}$ has been computed,
we are in a position to determine a column basis $\mathbf{T}$ using
the equation $\mathbf{F}=\mathbf{T}\mathbf{G}$. In order to do so
efficiently, however, the degree of $\mathbf{T}$ cannot be too large.
We see that this is the case from the following lemma. 
\begin{lem}
\label{lem:colBasisdegreeBoundByRdegOfRightFactor} Let $\mathbf{F}$
and $\mathbf{G}$ be as before and $\vec{t}=-\rdeg_{-\vec{s}}\mathbf{G}$.
Then 
\begin{itemize}
\item [(i)] the column degrees of $\mathbf{T}$ are bounded by the corresponding
entries of $\vec{t}$; 
\item [(ii)] if $\vec{t}$ has $r$ entries and $\vec{s}^{~\prime}$ is
the list of the $r$ largest entries of $\vec{s}$, then $\vec{t}\le\vec{s}^{~\prime}$. 
\end{itemize}
\end{lem}
\begin{proof}
Since $\mathbf{G}$ is $-\vec{s}$-row reduced, and $\rdeg_{-\vec{s}}\mathbf{F}\le0$,
by Lemma \ref{lem:predictableDegree} $\rdeg_{-\vec{t}}\mathbf{T}\le0$,
or equivalently, $\mathbf{T}$ has column degrees bounded by $\vec{t}$.

Let $\mathbf{G}^{\prime}$ be the $-\vec{s}$-row Popov form of $\mathbf{G}$
and the square matrix $\mathbf{G}^{\prime\prime}$ consist of only
the columns of $\mathbf{G}^{\prime}$ that contains pivot entries,
and has the rows permuted so the pivots are in the diagonal. Let $\vec{s}^{~\prime\prime}$
be the list of the entries in $\vec{s}$ that correspond to the columns
of $\mathbf{G}^{\prime\prime}$ in $\mathbf{G}^{\prime}$. Note that
$\rdeg_{-\vec{s}^{~\prime\prime}}\mathbf{G}^{\prime\prime}=-\vec{t}^{~\prime\prime}$
is just a permutation of $-\vec{t}$ with the same entries. By the
definition of shifted row degree, $-\vec{t}^{~\prime\prime}$ is the
sum of $-\vec{s}^{~\prime\prime}$ and the list of the diagonal pivot
degrees, which are nonnegative. Therefore, $-\vec{t}^{~\prime\prime}\ge-\vec{s}^{~\prime\prime}$.
The result then follows as $\vec{t}$ is a permutation of $\vec{t}^{~\prime\prime}$
and $\vec{s}^{\ \prime}$ consists of the largest entries of $\vec{s}$. 
\end{proof}
Having determined a bound on the column degrees of $\mathbf{T}$,
we are now ready to compute $\mathbf{T}$. This is done again by computing
a kernel basis using an order basis computation as before. 
\begin{lem}
Let $\vec{t}^{*}=\left[0,\dots,0,\vec{t}\right]\in\mathbb{Z}^{m+r}$.
Then any $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}^{*}\right)$-kernel
basis has the form $\begin{bmatrix}V\\
\bar{\mathbf{T}}
\end{bmatrix}$, where $V\in\mathbb{K}^{m\times m}$ is a unimodular matrix and $\left(\bar{\mathbf{T}}V^{-1}\right)^{T}$
is a column basis of $\mathbf{F}$. \end{lem}
\begin{proof}
Note first that the matrix $\begin{bmatrix}-I~\\
~\mathbf{T}^{T}
\end{bmatrix}$ is a kernel basis of $\left[\mathbf{F}^{T},\mathbf{G}^{T}\right]$
and is therefore unimodularly equivalent to any other kernel basis.
Hence any other kernel basis has the form $\begin{bmatrix}-I~\\
~\mathbf{T}^{T}
\end{bmatrix}U=\begin{bmatrix}V\\
\bar{\mathbf{T}}
\end{bmatrix}$, with $U$ and $V=-U$ unimodular. Thus $\mathbf{T}=\left(\bar{\mathbf{T}}V^{-1}\right)^{T}$.
Also note that the $-\vec{t}^{*}$ minimality forces the unimodular
matrix $V$ in any $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}^{*}\right)$-kernel
basis to be of degree 0, the same degree as $I$. \end{proof}
\begin{exmp}
Let 
\[
\mathbf{F}=\left[\begin{array}{cccc}
x^{2} & x^{2} & x+x^{2} & 1+x^{2}\\
1+x+x^{2} & x^{2} & 1+x^{2} & 1+x^{2}
\end{array}\right],
\]
a matrix over $\mathbb{Z}_{2}[x]$, and 
\[
\mathbf{G}=\left[\begin{array}{cccc}
1 & 0 & 1 & 0\\
\noalign{\medskip}x & {x}^{2} & 0 & 1+{x}^{2}
\end{array}\right],
\]
a minimal left kernel basis of a right kernel basis of $\mathbf{F}$.
In order to compute the column basis $\mathbf{T}$ satisfying $\mathbf{F}=\mathbf{T}\mathbf{G}$,
first we can determine $\cdeg\mathbf{T}\le\vec{t}=\left[2,0\right]$
from Lemma \ref{lem:colBasisdegreeBoundByRdegOfRightFactor}. Then
we can compute a $\left[0,0,-\vec{t}\right]$-minimal left kernel
basis of $\begin{bmatrix}\mathbf{F}\\
\mathbf{G}
\end{bmatrix}$. The matrix 
\[
\left[V,\bar{\mathbf{T}}\right]=\left[\begin{array}{cccc}
\noalign{\medskip}1 & 0 & x+{x}^{2} & 1\\
1 & 1 & 1+x & 0
\end{array}\right]
\]
is such a left kernel basis. A column basis can then be computed as
\[
\mathbf{T}=V^{-1}\bar{\mathbf{T}}=\left[\begin{array}{cc}
x+x^{2} & 1\\
1+{x}^{2} & 1
\end{array}\right].
\]
\qed 
\end{exmp}
In order to compute a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}^{*}\right)$-kernel
basis, we can again use order basis computation as before, as we again
have an order basis that contains a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}^{*}\right)$-kernel
basis. 
\begin{lem}
Any $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],\vec{s}+1,-\vec{t}^{*}\right)$-order
basis contains a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}^{*}\right)$-kernel
basis whose $-\vec{t}^{*}$-row degrees are bounded by 0. \end{lem}
\begin{proof}
As before, Lemma \ref{lem:orderbasisContainsKernelbasisGeneralized}
can be used here. We just need to show that a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}^{*}\right)$-kernel
basis has $-\vec{t}^{*}$-row degrees no more than $0$. This follows
from the fact that $\rdeg_{-\vec{t}^{*}}\begin{bmatrix}I\\
\mathbf{T}^{T}
\end{bmatrix}\le0$. 
\end{proof}
In order to compute a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}^{*}\right)$-kernel
basis efficiently, we notice that we have the same type of problem
as in Section \ref{sub:kernelBasisViaOrderBasisByRows} and hence
we can again use Algorithm \ref{alg:minimalKernelBasisReverse}. 
\begin{lem}
\label{lem:costOfKernelBasisReversedForLeftFactor}A $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}^{*}\right)$-kernel
basis can be computed using \prettyref{alg:minimalKernelBasisReverse}
with a cost of $O^{\sim}\left(n^{\omega}s\right)$, where $s=\xi/n$
is the average column degree of $\mathbf{F}$ as before. \end{lem}
\begin{proof}
Just use the algorithm with input $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],\vec{t}^{*},\xi\right)$.
We can verify the conditions on the input are satisfied. 
\begin{itemize}
\item To see that $\sum\rdeg_{\vec{t}^{*}}\left[\mathbf{F}^{T},\mathbf{G}^{T}\right]\le\xi$,
note that from $\vec{t}=-\rdeg_{-\vec{s}}\mathbf{G}$ and \prettyref{lem:columnDegreesRowDegreesSymmetry}
that $\cdeg_{\vec{t}}\mathbf{G}\le\vec{s}$, or equivalently, $\rdeg_{\vec{t}}\mathbf{G}^{T}\le\vec{s}$.
Since we also have $\rdeg\mathbf{F}^{T}\le\vec{s}$, it follows that
$\rdeg_{\vec{t}^{*}}\left[\mathbf{F}^{T},\mathbf{G}^{T}\right]\le\vec{s}$. 
\item The second condition $\sum\vec{t}^{*}\le\xi$ follows from \prettyref{lem:colBasisdegreeBoundByRdegOfRightFactor}. 
\item The third condition holds since $\begin{bmatrix}-I~\\
~\mathbf{T}^{T}
\end{bmatrix}$ is a kernel basis with row degrees bounded by $\vec{t}^{*}$. 
\end{itemize}
\end{proof}
With a $\left(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],-\vec{t}^{*}\right)$-kernel
basis $\begin{bmatrix}V\\
\bar{\mathbf{T}}
\end{bmatrix}$ computed, a column basis is then given by $\mathbf{T}~=~\left(\bar{\mathbf{T}}V^{-1}\right)^{T}$.

The complete algorithm for computing a column basis is then given
in Algorithm \ref{alg:colBasis}.

\begin{algorithm}[t]
\caption{$\colBasis(\mathbf{F})$}


\label{alg:colBasis}

\smallskip{}


\begin{algorithmic}[1]
\REQUIRE{$\mathbf{F}\in\mathbb{K}\left[x\right]^{m\times n}$.}

\ENSURE{a column basis of $\mathbf{F}$.}

\smallskip{}


\STATE{$\vec{s}~~~:=\cdeg\mathbf{F}$;}

\STATE{$\mathbf{N}~~:=\mnb(\mathbf{F},\vec{s})$;}

\STATE{$\mathbf{G}:=\left(\mnbr(\mathbf{N}^{T},\vec{s},\sum\vec{s})\right)^{T}$; }

\STATE{$\vec{t}^{*}:=\left[0,\dots,0,-\rdeg_{-\vec{s}}\mathbf{G}\right]$,
with $\rowDimension(\mathbf{G})$ $~~~~~~~~~~~$ number of 0's ; }

\STATE{$\left[\begin{array}{c}
V\\
\bar{\mathbf{T}}
\end{array}\right]:=\mnbr(\left[\mathbf{F}^{T},\mathbf{G}^{T}\right],\vec{t}^{*},\sum\vec{s})$ $~~~~~~~~~~~~~~~~$ with a square $V$; }

\STATE{\textbf{$\mathbf{T}~~=\left(\bar{\mathbf{T}}V^{-1}\right)^{T}$};}

\RETURN $\mathbf{T}$; 
\end{algorithmic}
\end{algorithm}

\begin{thm}
\label{thm:columnBasisCost1}A column basis $\mathbf{T}$ of $\mathbf{F}$
can be computed with a cost of $O^{\sim}\left(n^{\omega}s\right)$,
where $s=\xi/n$ is the average column degree of $\mathbf{F}$ as
before. \end{thm}
\begin{proof}
The cost is dominated by the cost of the three kernel basis computations
in the algorithm. The first one is handled by the algorithm from \cite{za2012}
and \prettyref{thm:costGeneral}, while the remaining two are handled
by \prettyref{alg:minimalKernelBasisReverse}, \prettyref{lem:costKernelBasisReverse}
and \prettyref{lem:costOfKernelBasisReversedForLeftFactor}.
\end{proof}

\section{\label{sec:successiveColBasisComputation}A Simple Improvement}

When the input matrix $\mathbf{F}$ has column dimension $n$ much
larger than the row dimension $m$, then we can separate $\mathbf{F}=\left[\mathbf{F}_{1},\mathbf{F}_{2},\dots,\mathbf{F}_{n/m}\right]$
into $n/m$ blocks, each with dimension $m\times m$, assuming without
loss of generality $n$ is a multiple of $m$, and the columns are
arranged in increasing degrees. We then do a series of column basis
computations. First we compute a column basis $\mathbf{T}_{1}$ of
$\left[\mathbf{F}_{1},\mathbf{F}_{2}\right]$. Then compute a column
basis $\mathbf{T}_{2}$ of $\left[\mathbf{T}_{1},\mathbf{F}_{3}\right]$.
Repeating this process, at step $i$, we compute a column basis $\mathbf{T}_{i}$
of $\left[\mathbf{T}_{i-1},\mathbf{F}_{i+1}\right]$, until $i=n/m-1$,
when a column basis of $\mathbf{F}$ is computed. 
\begin{lem}
Let $\bar{s}_{i}=\left(\sum\cdeg\mathbf{F}_{i}\right)/m$. Then at
step $i$, computing a column basis $\mathbf{T}_{i}$ of $\left[\mathbf{T}_{i-1},\mathbf{F}_{i+1}\right]$
can be done with a cost of $O^{\sim}\left(m^{\omega}(\bar{s}_{i}+\bar{s}_{i+1})/2\right)$
field operations.\end{lem}
\begin{proof}
From Lemma \ref{lem:colBasisdegreeBoundByRdegOfRightFactor}, the
column basis $\mathbf{T}_{i-1}$ of \\
$\left[\mathbf{F}_{1},\dots,\mathbf{F}_{i}\right]$ has column degrees
bounded by the largest column degrees of $\mathbf{F}_{i}$, hence
$\sum\cdeg\mathbf{T}_{i-1}\le\sum\cdeg\mathbf{F}_{i}$. The lemma
then follows by combining this with the result from Theorem \ref{thm:columnBasisCost1}
that a column basis $\mathbf{T}_{i}$ of $\left[\mathbf{T}_{i-1},\mathbf{F}_{i+1}\right]$
can be computed with a cost of $O^{\sim}\left(m^{\omega}\hat{s}_{i}\right)$,
where 
\[
\hat{s}_{i}=\left(\sum\cdeg\mathbf{T}_{i-1}+\sum\cdeg\mathbf{F}_{i+1}\right)/2m\le\frac{\left(\bar{s}_{i}+\bar{s}_{i+1}\right)}{2}.
\]
\end{proof}
\begin{thm}
If $s=\left(\sum\cdeg\mathbf{F}\right)/n$, then \label{thm:finalCollBasisCost}
a column basis of $\mathbf{F}$ can be computed with a cost of $O^{\sim}\left(m^{\omega}s\right)$. \end{thm}
\begin{proof}
Summing up the cost of all the column basis computations, 
\begin{eqnarray*}
 &  & \sum_{i=1}^{n/m-1}O^{\sim}\left(m^{\omega}\left(\bar{s}_{i}+\bar{s}_{i+1}\right)/2\right)\\
 &  & ~~~\subset O^{\sim}\left(m^{\omega}\left(\sum_{i=1}^{n/m}\bar{s}_{i}\right)\right)=O^{\sim}\left(nm^{\omega-1}s\right),
\end{eqnarray*}
since $\sum\cdeg\mathbf{F}=\sum_{i=1}^{n/m}\left(m\bar{s}_{i}\right)=ns.$\end{proof}
\begin{rem}
In this section, the computational efficiency is improved by reducing
the original problem to about $n/m$ subproblems whose column dimensions
are close to the row dimension $m$. This is done by successive column
basis computations. Note that we can also reduce the column dimension
by using successive order basis computations, and only do a column
basis computation at the very last step. The computational complexity
of using order basis computation to reduce the column dimension would
remain the same, but in practice it may be more efficient since order
basis computations are simpler. 
\end{rem}

\section{Conclusion}

In this paper we have given a fast, deterministic algorithm for the
computation of a column basis for $\mathbf{F}$ having complexity
$O^{\sim}\left(n^{\omega}s\right)$ field operations in $\mathbb{K}$
with $s$ an upper bound for the average column degree of $\mathbf{F}$.
Our methods rely on a special factorization of $\mathbf{F}$ into
a column basis and a kernel basis. These in turn are computed via
fast kernel basis and fast order basis algorithm of \cite{za2012,ZL2012}.
When these computations involve the multiplication of polynomial matrices
with unbalanced degrees then they use the fast method for such multiplications
given in \cite{za2012}.

%For some applications of column basis computation, including efficient
%deterministic computations of matrix determinant, Hermite form, and
%computations of column reduced form and Popov form for matrices of
%any rank and any dimension, we refer the readers to the thesis \cite{zhou:phd2012}.


% 
In a later publication we will show how this column basis algorithm
can be used in efficient deterministic computations of matrix determinant,
Hermite form, and computations of column reduced form and Popov form
for matrices of any rank and any dimension.

%\bibliographystyle{plainnat}
 \bibliographystyle{plain}
\bibliography{paper}

\end{document}
